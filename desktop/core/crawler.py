"""
YTN ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
YTN ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
"""

try:
    import requests
    from bs4 import BeautifulSoup
    WEB_SCRAPING_AVAILABLE = True
except ImportError:
    print("ì›¹ ìŠ¤í¬ë ˆì´í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install requests beautifulsoup4")
    WEB_SCRAPING_AVAILABLE = False

import re
import time
import random
from datetime import datetime
from typing import List, Dict

from PyQt5.QtCore import QThread, pyqtSignal

class CrawlerThread(QThread) :

    progress_updated = pyqtSignal(str)
    crawling_finished = pyqtSignal(list)

    def __init__(self):
        super().__init__()  # ì´ ì¤„ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤
        self.target_count = 1  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì„
        self.delay_range = (0.5, 1)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì„
        print("1 - ì´ˆê¸°í™” ì™„ë£Œ")    


    def run(self):  # QThreadì˜ í‘œì¤€ ë©”ì„œë“œ
        """ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œ"""
        if not WEB_SCRAPING_AVAILABLE:
            print("2 - ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")    
            self.progress_updated.emit("âŒ ì›¹ ìŠ¤í¬ë ˆì´í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.crawling_finished.emit([])
            return
        
        try:
            print("4 - í¬ë¡¤ë§ ì‹œì‘")  
            news_list = self.crawl_ytn_news()
            print("5 - í¬ë¡¤ë§ ì™„ë£Œ")  
            self.crawling_finished.emit(news_list)  # ì‹œê·¸ë„ë¡œ ê²°ê³¼ ì „ë‹¬
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            self.progress_updated.emit(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            self.crawling_finished.emit([])


    def emit_progress(self, message):
        """ì§„í–‰ìƒí™© ì¶œë ¥ (ì‹¤ì œë¡œëŠ” ì‹œê·¸ë„ emit)"""
        print(f"Progress: {message}")
        self.progress_updated.emit(message)  # ì´ ì¤„ ì¶”ê°€

    def crawl_ytn_news(self) -> List[Dict]:
        """YTN ë‰´ìŠ¤ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        self.emit_progress("ğŸŒ YTN ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ëŠ” ì¤‘...")
        news_list = []
        
        try:
            print("6 - URL ìˆ˜ì§‘ ì‹œì‘")  
            news_urls = self.get_news_urls()
            print("10 - URL ìˆ˜ì§‘ ì™„ë£Œ") 
            
            if not news_urls:
                self.emit_progress("âš ï¸ ë‰´ìŠ¤ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            
            self.emit_progress(f"ğŸ“° {len(news_urls)}ê°œ ë‰´ìŠ¤ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
            # ê° ë‰´ìŠ¤ ì²˜ë¦¬
            for i, url in enumerate(news_urls[:self.target_count]):
                self.emit_progress(f"ğŸ“„ ë‰´ìŠ¤ {i+1}/{min(len(news_urls), self.target_count)} ìˆ˜ì§‘ ì¤‘...")
                
                try:
                    print("11-1. ë‰´ìŠ¤ ì²˜ë¦¬ ì‹œì‘")
                    print(f'0. url : {url}')
                    news_list = self.crawl_single_news(url)
                    print("20 - news ìˆ˜ì§‘ ì™„ë£Œ") 
                    print(f'21. news_list : {news_list}')

                    if news_list:

                        self.emit_progress(f"âœ… '{len(news_list)}'ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                
                except Exception as e:
                    print("11-2. ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜")
                    self.emit_progress(f"âŒ ë‰´ìŠ¤ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                # ì§€ì—°

                time.sleep(random.uniform(*self.delay_range))
                
            self.emit_progress(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(news_list)}ê°œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return news_list
            
        except Exception as e:
            self.emit_progress(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return news_list
        
    def get_news_urls(self) -> List[str]:
        """YTNì—ì„œ ë‰´ìŠ¤ URL ëª©ë¡ ì¶”ì¶œ"""

        try:
            headers = self.get_headers()
            
            # YTN ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            print("7 - ë©”ì¸ í˜ì´ì§€ ") 
            response = requests.get('https://www.ytn.co.kr', timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_urls = []
            
            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì„¹ì…˜ì—ì„œ ë§í¬ ì¶”ì¶œ
            selectors = [
                '.top_menu > .menu > li > a'
            ]
            
            for selector in selectors[:]:
                links = soup.select(selector)
                # print(f'links: {links}')
                for link in links:
                    href = link.get('href', '')
                    print(f'href : {href}')
                    news_urls.append(href)
            
            return news_urls  # ìµœëŒ€ 15ê°œ ë°˜í™˜
            
        except Exception as e:
            print(f"ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
        
    def crawl_single_news(self, url: str) -> list:
        """ê°œë³„ ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            print("12-1. ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            headers = self.get_headers()
            print(f'headers : {headers}')
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            print("12-2. ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            print(f'crawl_url : {url}')
            # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ

            news_list = [
                self.extract_category(url, soup),
                self.extract_title(soup),
                self.extract_content(soup),
                self.extract_news_url(soup),
                [False],
                [''],
                self.extract_published_date(soup)
            ]
            
            print(f'news_list[category] : {news_list[0]}')
            print(f'news_list[title] : {news_list[1]}')
            print(f'news_list[content] : {news_list[2]}')
            print(f'news_list[url] : {news_list[3]}')
            print(f'news_list[published_date] : {news_list[6]}')
            
            final_news_list = []
            max_length = len(news_list[0]) if news_list[0] else 0

            for i in range(max_length):
                news = {
                    'no': '',
                    'category': news_list[0][i] if i < len(news_list[0]) else '',
                    'title': news_list[1][i] if i < len(news_list[1]) else '',
                    'content': news_list[2][i] if i < len(news_list[2]) else '',
                    'url': news_list[3][i] if i < len(news_list[3]) else '',
                    'posted_to_blog': news_list[4][i] if i < len(news_list[4]) else False,
                    'blog_url': news_list[5][i] if i < len(news_list[5]) else '',
                    'published_date': news_list[6][i] if i < len(news_list[6]) else '',
                }
                final_news_list.append(news)

            print(f'news_list : {final_news_list}')

            print("13. ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ")

            return final_news_list

        except Exception as e:
            print(f"ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
            return None

    def get_headers(self) -> Dict:
        """HTTP ìš”ì²­ í—¤ë” ë°˜í™˜"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59'
        ]
        
        return {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

    def extract_title(self, soup: BeautifulSoup) :
        """ì œëª© ì¶”ì¶œ"""
        selectors = ['.news_list_wrap > .news_list > .text_area > .title > a']
        titles = []

        print("111. ì œëª© ì¶”ì¶œ ì‹œì‘") 
        for selector in selectors:
            elements = soup.select(selector)

            if elements :
                print("112. ì œëª© ì¶”ì¶œ ì‹œì‘")  
                for element in elements :
                    title = element.get_text()
                    titles.append(title)        

        titles = self.clean_content(titles)

        print(titles)
        print("=========================================================================================")

        return titles
    
    def extract_content(self, soup: BeautifulSoup) :
        """ë³¸ë¬¸ ì¶”ì¶œ"""

        print("222. ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘")
        selectors = ['.news_list_wrap > .news_list > .text_area > .content' ]
        contents = []

        for selector in selectors:
            elements = soup.select(selector)

            if elements :
                for element in elements :    
                    content = element.get_text()
                    
                    contents.append(content)
        # ë‚´ìš© ì •ì œ
        
        contents = self.clean_content(contents)

        print(contents)
        print("=========================================================================================")

        return contents if contents else "ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def extract_category(self, url, soup: BeautifulSoup) :
        """ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        extract_count = 10

        category_url = url
        selector = f'.top_menu > .menu > li > a[href="{category_url}"]'
        
        print("ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œì‘1")
        element = soup.select_one(selector)
        categorys = []

        print("ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œì‘2")
        if element:
            category = element.get_text()
            print(category)
            for i in range(0, extract_count) :
                categorys.append(category)
        else :
            print("ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì˜¤ë¥˜")
        
        print(categorys)
        print("=========================================================================================")

        return categorys   

    
    def extract_published_date(self, soup: BeautifulSoup) :
        """ë°œí–‰ì¼ ì¶”ì¶œ"""

        selectors = ['.news_list_wrap > .news_list > .text_area > .info > .date']
        date_texts = []
        print("14. ë°œí–‰ì¼ ì¶”ì¶œ ì‹œì‘ ")

        for selector in selectors:
            elements = soup.select(selector)

            if elements:
                for element in elements :    
                    date_text = element.get_text()
                    date_texts.append(date_text)

        print(date_texts)
        print("=========================================================================================")

        return date_texts

    def extract_news_url(self, soup: BeautifulSoup) :
        """ë‰´ìŠ¤ URL ì¶”ì¶œ"""
        print("15-1. ë‰´ìŠ¤ url ì‹œì‘ ")

        selectors = ['.news_list_wrap > .news_list > .text_area > .title > a']

        print("15-2. ë‰´ìŠ¤ url ì‹œì‘ ")

        news_urls = []
        for selector in selectors:
            elements = soup.select(selector)

            if elements:
                for element in elements :    
                    news_url = element.get('href')
                    news_urls.append(news_url)

        print(news_urls)
        print("=========================================================================================")

        return news_urls

    def clean_content(self, contents: list) -> list :
        """ë³¸ë¬¸ ë‚´ìš© ì •ì œ"""

        cleaned_contents = []
        print("ë³¸ë¬¸ ë‚´ìš© ì •ì œ ì‹œì‘")

        if not contents:
            return ""
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
        unwanted_patterns = [
            r'â–¶.*?â—€',  # YTN íŠ¹ìˆ˜ ê¸°í˜¸ íŒ¨í„´
            r'â€».*?â€»',  # ì£¼ì„ íŒ¨í„´
            r'\[.*?\]',  # ëŒ€ê´„í˜¸ íŒ¨í„´
            r'<.*?>',   # HTML íƒœê·¸
            r'&[^;]+;', # HTML ì—”í‹°í‹°
            r'ê´‘ê³ |ë°°ë„ˆ|í´ë¦­|ë°”ë¡œê°€ê¸°|ë”ë³´ê¸°',  # ê´‘ê³  ê´€ë ¨
            r'ê¸°ì\s*=|ë‰´ìŠ¤\d+',  # ê¸°ì ì„œëª…
            r'â“’.*?YTN.*',  # ì €ì‘ê¶Œ í‘œì‹œ
            r'www\.ytn\.co\.kr',  # URL
            r'\s{2,}'  # ì—°ì†ëœ ê³µë°±
            r'\s+' # ì—°ì† ê³µë°± ì œê±°
            r'\\' # \ ë¬¸ì íŒ¨í„´
        ]
        
        for content in contents :
            for pattern in unwanted_patterns:
                content = re.sub(pattern, ' ', content, flags=re.IGNORECASE)
        
            # ì¶”ê°€ ì •ì œ
                cleaned_content = content.strip()
            cleaned_contents.append(cleaned_content)

        # ê¸¸ì´ ì œí•œ (500ì)
        
        return cleaned_contents

    async def get_status(self):
        return {
            'status': 'ready',
            'service': 'crawler_thread',
            'last_run': None,
            'total_crawled': 0,
            'success_rate': 1.0
        }

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":

    print("=== í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    crawler = CrawlerThread()
    result = crawler.run()
    
    print("\n=== ê²°ê³¼ ===")
    for i, news in enumerate(result, 1):
        print(f"{i}. {news['title']}")
    
    print(f"\nì´ {len(result)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ë¨")