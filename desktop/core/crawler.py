"""
YTN ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ë³‘ë ¬ ì²˜ë¦¬ ì•ˆì „ ë²„ì „)
YTN ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
"""

try:
    import requests
    from bs4 import BeautifulSoup
    WEB_SCRAPING_AVAILABLE = True
except ImportError:
    print("ì›¹ ìŠ¤í¬ë ˆì´í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install requests beautifulsoup4")
    WEB_SCRAPING_AVAILABLE = False

import re
import time
import random
from datetime import datetime
from typing import List, Dict
import json
import concurrent.futures
import threading

from PyQt5.QtCore import QThread, pyqtSignal

class CrawlerThread(QThread):

    progress_updated = pyqtSignal(str)
    crawling_finished = pyqtSignal(list)

    def __init__(self):
        super().__init__()
        self.target_count = 10
        self.delay_range = (0.1, 0.3)  # ì§€ì—°ì‹œê°„ ë‹¨ì¶•: 0.5-1ì´ˆ â†’ 0.1-0.3ì´ˆ
        
        # âœ… ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•œ ë½ ì¶”ê°€
        self.progress_lock = threading.Lock()
        self.counter_lock = threading.Lock()
        self.completed_count = 0
        
        print("1 - ì´ˆê¸°í™” ì™„ë£Œ")    

    def run(self):  # QThreadì˜ í‘œì¤€ ë©”ì„œë“œ
        """ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œ"""
        if not WEB_SCRAPING_AVAILABLE:
            self.emit_progress("âŒ ì›¹ ìŠ¤í¬ë ˆì´í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.crawling_finished.emit([])
            return
        
        try:
            news_list = self.crawl_ytn_news()
            self.crawling_finished.emit(news_list)  # ì‹œê·¸ë„ë¡œ ê²°ê³¼ ì „ë‹¬
        except Exception as e:
            error_msg = f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}"
            print(error_msg)
            self.emit_progress(error_msg)
            self.crawling_finished.emit([])

    def emit_progress(self, message):
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ì§„í–‰ìƒí™© ì¶œë ¥"""
        # ìŠ¤ë ˆë“œ ID í¬í•¨í•˜ì—¬ ë””ë²„ê¹… ê°€ëŠ¥í•˜ê²Œ ìˆ˜ì •
        thread_id = threading.current_thread().ident
        
        with self.progress_lock:
            timestamped_message = f"[{datetime.now().strftime('%H:%M:%S')}] {message}"
            print(f"[Thread-{thread_id}] {timestamped_message}")
            self.progress_updated.emit(timestamped_message)

    def crawl_ytn_news(self) -> List[Dict]:
        """YTN ë‰´ìŠ¤ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        self.emit_progress("ğŸŒ YTN ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ëŠ” ì¤‘...")
        news_list = []
        
        try:
            news_urls = self.get_news_urls()
            print(f'news_urls : {news_urls}')
          
            if not news_urls:
                self.emit_progress("âš ï¸ ë‰´ìŠ¤ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            self.emit_progress(f"ğŸ“° {len(news_urls)}ê°œ ë‰´ìŠ¤ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
            # âœ… ì¹´ìš´í„° ì´ˆê¸°í™”
            with self.counter_lock:
                self.completed_count = 0
                 
            # === ë³‘ë ¬ì²˜ë¦¬ë¡œ ê° ë‰´ìŠ¤ ìˆ˜ì§‘ (ìŠ¤ë ˆë“œ ì•ˆì „ ë²„ì „) ===
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # ë™ì‹œ ìŠ¤ë ˆë“œ ìˆ˜ ì¤„ì„
                # ê° URLì— ëŒ€í•´ future ìƒì„±
                futures = []
                target_urls = news_urls[:self.target_count]
                
                for i, url in enumerate(target_urls):
                    self.emit_progress(f"ğŸ“„ ë‰´ìŠ¤ {i+1}/{len(target_urls)} ìˆ˜ì§‘ ì‹œì‘...")
                    future = executor.submit(self.crawl_single_news_safe, url, i+1)
                    futures.append(future)
                
                # âœ… ê²°ê³¼ë¥¼ ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ ìˆ˜ì§‘
                results = []
                for future in concurrent.futures.as_completed(futures):
                    try:
                        news = future.result()
                        if news:
                            results.append(news)
                            
                            # âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ì¹´ìš´í„° ì—…ë°ì´íŠ¸
                            with self.counter_lock:
                                self.completed_count += 1
                                current_count = self.completed_count
                            
                            self.emit_progress(f"âœ… ë‰´ìŠ¤ {current_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                    except Exception as e:
                        self.emit_progress(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                
                # âœ… ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ê°€
                news_list.extend(results)
                
            self.emit_progress(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(news_list)}ê°œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return news_list
            
        except Exception as e:
            self.emit_progress(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return news_list
        
    def get_news_urls(self) -> List[str]:
        """YTNì—ì„œ ë‰´ìŠ¤ URL ëª©ë¡ ì¶”ì¶œ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        # âœ… ëª¨ë“  ë³€ìˆ˜ë¥¼ ë¡œì»¬ë¡œ ì²˜ë¦¬
        api_url = 'https://www.ytn.co.kr/ajax/getManyNews.php'
    
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Origin': 'https://www.ytn.co.kr',
            'Referer': 'https://www.ytn.co.kr/',
            'X-Requested-With': 'XMLHttpRequest',
            'Sec-Ch-Ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Priority': 'u=1, i'
        }
        
        # Form Data
        form_data = {
            'mcd': 'total'
        }
        
        try:
            start_time = time.time()
            
            # POST ìš”ì²­ (Form Dataì™€ í•¨ê»˜)
            response = requests.post(api_url, headers=headers, data=form_data, timeout=10)
            response.raise_for_status()
            
            # ì‘ë‹µ ë””ë²„ê¹…
            print(f"ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"Content-Type: {response.headers.get('Content-Type')}")
            print(f"ì‘ë‹µ ê¸¸ì´: {len(response.text)}")
            print(f"ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 100ì): {response.text[:100]}")
            
            # ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ nullì¸ ê²½ìš° ì²´í¬
            if not response.text or response.text.strip() in ['null', '', 'false']:
                print("âŒ ë¹ˆ ì‘ë‹µ ë˜ëŠ” null ì‘ë‹µ")
                return []
            
            # JSON íŒŒì‹± (í•œêµ­ì–´ ìœ ë‹ˆì½”ë“œ ìë™ ë””ì½”ë”©)
            try:
                articles = response.json()
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                print(f"ì‘ë‹µ ì „ì²´ ë‚´ìš©: {response.text}")
                return []
            
            # articlesê°€ Noneì¸ì§€ ì²´í¬
            if articles is None:
                print("âŒ articlesê°€ Noneì…ë‹ˆë‹¤")
                return []
            
            # articlesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ì²´í¬
            if not isinstance(articles, list):
                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ íƒ€ì…: {type(articles)}")
                print(f"ì‘ë‹µ ë‚´ìš©: {articles}")
                return []
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print(f"í¬ë¡¤ë§ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            print(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")
            print("-" * 60)
            
            # âœ… ë¡œì»¬ ë³€ìˆ˜ë¡œ URL ë¦¬ìŠ¤íŠ¸ ìƒì„±
            news_urls = []
            for i, article in enumerate(articles[:10], 1):
                join_key = article.get('join_key', '')
                mcd = article.get('mcd', '')
                
                # YTN ê¸°ì‚¬ URL ìƒì„±
                news_url = f"https://www.ytn.co.kr/_ln/{mcd}_{join_key}"
                news_urls.append(news_url)
                
            return news_urls
            
        except requests.RequestException as e:
            print(f"ìš”ì²­ ì—ëŸ¬: {e}")
            return []
        except Exception as e:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬: {e}")
            return []

    def crawl_single_news_safe(self, url: str, index: int) -> dict:
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ê°œë³„ ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§"""
        thread_id = threading.current_thread().ident
        
        try:
            self.emit_progress(f"[Thread-{thread_id}] ë‰´ìŠ¤ {index} ì²˜ë¦¬ ì‹œì‘: {url}")
            
            # âœ… ëª¨ë“  ë³€ìˆ˜ë¥¼ ë¡œì»¬ë¡œ ìƒì„±
            headers = self.get_headers()
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # âœ… ê° ì¶”ì¶œ ë©”ì„œë“œë¥¼ ë…ë¦½ì ìœ¼ë¡œ í˜¸ì¶œí•˜ê³  ë¡œì»¬ ë³€ìˆ˜ì— ì €ì¥
            local_category = self.extract_category(url, soup, thread_id)
            local_title = self.extract_title(soup, thread_id)
            local_content = self.extract_content(soup, thread_id)
            local_published_date = self.extract_published(soup, thread_id)
            
            # âœ… authorì™€ email ì¶”ì¶œë„ ë¡œì»¬ ë³€ìˆ˜ë¡œ
            local_author = self.extract_author(local_content, thread_id)
            local_email = self.extract_email(local_content, thread_id)
            
            # âœ… ë‰´ìŠ¤ ê°ì²´ë¥¼ ë¡œì»¬ ë³€ìˆ˜ë¡œë§Œ êµ¬ì„±
            news = {
                'category': local_category,
                'title': local_title,
                'content': local_content,
                'author': local_author,
                'email': local_email,
                'url': url,
                'posted_to_blog': False,
                'blog_url': '',
                'published_date': local_published_date,
                'thread_id': thread_id,  # ë””ë²„ê¹…ìš© ìŠ¤ë ˆë“œ ID í¬í•¨
                'index': index  # ë””ë²„ê¹…ìš© ì¸ë±ìŠ¤ í¬í•¨
            }
            
            self.emit_progress(f"[Thread-{thread_id}] ë‰´ìŠ¤ {index} ì™„ë£Œ: {local_title[:50]}...")
            return news

        except Exception as e:
            error_msg = f"[Thread-{thread_id}] ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}"
            print(error_msg)
            self.emit_progress(error_msg)
            return None

    def get_headers(self) -> Dict:
        """HTTP ìš”ì²­ í—¤ë” ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        # âœ… ë¡œì»¬ ë³€ìˆ˜ë¡œ ì²˜ë¦¬
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59'
        ]
        
        return {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

    def extract_title(self, soup: BeautifulSoup, thread_id: int):
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ì œëª© ì¶”ì¶œ"""
        selector = '.news_title_wrap > .news_title > span:last-child'
        element = soup.select_one(selector)

        if element:
            # âœ… ëª¨ë“  ì²˜ë¦¬ë¥¼ ë¡œì»¬ ë³€ìˆ˜ë¡œ
            local_title = element.get_text()
            local_title = self.clean_content(local_title)
            print(f'[Thread-{thread_id}] title : {local_title}')
            return local_title
        
        return "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    
    def extract_content(self, soup: BeautifulSoup, thread_id: int):
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ë³¸ë¬¸ ì¶”ì¶œ"""
        selector = '#CmAdContent > span'
        element = soup.select_one(selector)

        if element:
            # âœ… ëª¨ë“  ì²˜ë¦¬ë¥¼ ë¡œì»¬ ë³€ìˆ˜ë¡œ
            local_content = element.get_text()
            local_content = self.clean_content(local_content)
            print(f'[Thread-{thread_id}] content length: {len(local_content)}')
            return local_content
                
        return "ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def extract_category(self, url, soup: BeautifulSoup, thread_id: int):
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        selector = '.top_menu > .menu > li.on > a'
        element = soup.select_one(selector)

        if element:
            # âœ… ë¡œì»¬ ë³€ìˆ˜ë¡œ ì²˜ë¦¬
            local_category = element.get_text()
            print(f'[Thread-{thread_id}] category : {local_category}')
            return local_category
        else:
            print(f"[Thread-{thread_id}] ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì˜¤ë¥˜")
            return "ê¸°íƒ€"
    
    def extract_published(self, soup: BeautifulSoup, thread_id: int):
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ë°œí–‰ì¼ ì¶”ì¶œ"""
        selector = '.news_title_wrap inner > .news_info > .date'
        element = soup.select_one(selector)

        if element:
            # âœ… ë¡œì»¬ ë³€ìˆ˜ë¡œ ì²˜ë¦¬
            local_date_text = element.get_text()
            print(f'[Thread-{thread_id}] date_text : {local_date_text}')
            return local_date_text
        
        # âœ… ê¸°ë³¸ê°’ë„ ë¡œì»¬ì—ì„œ ìƒì„±
        default_date = datetime.now().strftime("%Y-%m-%d %H:%M")
        return default_date

    def extract_author(self, content, thread_id: int):
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ê¸°ìëª… ì¶”ì¶œ"""
        # âœ… ëª¨ë“  ë³€ìˆ˜ë¥¼ ë¡œì»¬ë¡œ ì²˜ë¦¬
        local_text = content
        
        print(f'[Thread-{thread_id}] author ì¶”ì¶œ ì‹œì‘, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(local_text)}')
        
        # âœ… ê°œì„ ëœ íŒ¨í„´ìœ¼ë¡œ ì •í™•í•œ ë§¤ì¹­
        patterns = [
            r'YTN(?:\s+[a-z]+)?\s+([ê°€-í£]{2,4})(?=ì…ë‹ˆë‹¤|ì˜€ìŠµë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ê¸°ì|\s|$)',  # ì „ë°©íƒìƒ‰
            r'YTN(?:\s+[a-z]+)?\s+([ê°€-í£]{2,4})',  # ì¼ë°˜ íŒ¨í„´
        ]
        
        local_author = None
        for pattern in patterns:
            author_match = re.search(pattern, local_text)
            if author_match:
                local_author = author_match.group(1)
                print(f'[Thread-{thread_id}] íŒ¨í„´ ë§¤ì¹­: "{pattern}" â†’ "{local_author}"')
                break
        
        if not local_author:
            print(f'[Thread-{thread_id}] author ë§¤ì¹­ ì‹¤íŒ¨')
            return None
        
        # âœ… ì ‘ë¯¸ì‚¬ ì œê±°ë„ ë¡œì»¬ ë³€ìˆ˜ë¡œ
        original_author = local_author
        suffixes = ['ì…ë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ê¸°ìì…ë‹ˆë‹¤', 'ê¸°ì', 'ì…', 'ì˜€']
        
        for suffix in suffixes:
            if local_author.endswith(suffix):
                local_author = local_author[:-len(suffix)]
                print(f'[Thread-{thread_id}] ì ‘ë¯¸ì‚¬ "{suffix}" ì œê±°: "{original_author}" â†’ "{local_author}"')
                break
        
        # âœ… ì¶”ê°€ ê²€ì¦ - ì·¨ì¬ ê´€ë ¨ ë‹¨ì–´ ì œì™¸
        forbidden_words = ['ì·¨ì¬', 'ì·¨ì¬ì§„', 'ì·¨ì¬íŒ€', 'ì·¨ì¬ë¶€', 'í¸ì§‘', 'ì œì‘', 'ê¸°íš']
        if local_author in forbidden_words:
            print(f'[Thread-{thread_id}] ê¸ˆì§€ëœ ë‹¨ì–´ ì œì™¸: "{local_author}"')
            return None
        
        # âœ… ê²€ì¦: 2-4ê¸€ì í•œê¸€ ì´ë¦„ë§Œ
        if 2 <= len(local_author) <= 4 and re.match(r'^[ê°€-í£]+$', local_author):
            print(f'[Thread-{thread_id}] author ìµœì¢… ê²°ê³¼: {local_author}')
            return local_author
        
        print(f'[Thread-{thread_id}] author ê²€ì¦ ì‹¤íŒ¨: "{local_author}" (ê¸¸ì´: {len(local_author)})')
        return None

    def extract_email(self, content, thread_id: int):
        """âœ… ìŠ¤ë ˆë“œ ì•ˆì „ ì´ë©”ì¼ ì¶”ì¶œ"""
        # âœ… ëª¨ë“  ë³€ìˆ˜ë¥¼ ë¡œì»¬ë¡œ ì²˜ë¦¬
        local_text = content
        
        print(f'[Thread-{thread_id}] email ì¶”ì¶œ ì‹œì‘')
        
        # âœ… ì´ë©”ì¼ íŒ¨í„´ ë§¤ì¹­
        email_match = re.search(r'\(([a-zA-Z0-9._%+-]+@ytn\.co\.kr)\)', local_text)
        local_email = email_match.group(1) if email_match else None
        
        print(f'[Thread-{thread_id}] email ê²°ê³¼: {local_email}')
        return local_email

    def clean_content(self, content):
        """ë³¸ë¬¸ ë‚´ìš© ì •ì œ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        if not content:
            return ""
        
        # âœ… ë¡œì»¬ ë³€ìˆ˜ë¡œ ì²˜ë¦¬
        cleaned = re.sub(r'\s+', ' ', content).strip()
        return cleaned

    async def get_status(self):
        """ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'status': 'ready',
            'service': 'crawler_thread',
            'last_run': None,
            'total_crawled': 0,
            'success_rate': 1.0
        }

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("=== ìŠ¤ë ˆë“œ ì•ˆì „ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    crawler = CrawlerThread()
    
    result = crawler.crawl_ytn_news()
    
    print("\n=== ê²°ê³¼ ===")
    if result:
        for i, news in enumerate(result, 1):
            print(f"{i}. [{news.get('thread_id', 'Unknown')}] {news['title']} - {news['author']}")
        print(f"\nì´ {len(result)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ë¨")
    else:
        print("í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")